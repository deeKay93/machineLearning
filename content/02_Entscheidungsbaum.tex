\newpage
\section{Entscheidungsbaum}\label{sec:entscheidungsbaum}
\todoask[inline]{Vorteile Entscheidungsbaum? / Einleitung?}

\subsection{Konzept}
Grundlegend basiert der Algorithmus zum Erzeugen des Entscheidungsbaums auf dem,
in der Vorlesung vorgestellten. An verschiedenen Stellen wird der Algorithmus optimiert.
Der finale Algorithmus ist in \autoref{alg:baum-erzeugen} dargestellt.
Besonderer Augenmerk liegt hierbei auf den Optimierungen, welche im Folgenden beschrieben werden:
\begin{description}
    \item[\autoref{alg:baum-erzeugen-a}] Anstatt einer binären \enquote{Ja-Nein}-Klassifikation wird als Ergebnis die Wahrscheinlichkeit dafür zurückgegben,
                    dass für den entsprechenden Datensatz das Ergebnis positiv ist.
                    \todoask[inline]{Vorteil Wahrscheinlichkeit?}

    \item[\autoref{alg:baum-erzeugen-b}] Für den Fall, dass die Beispielmenge zwei (oder mehr) identische Datensätze mit unterschiedlicher Klassifikation enthält,
    jedoch die Attributmenge leer ist, wird der Anteil der positiven Klassifikationen als Wert für den entsprechenden Knoten gesetzt.
    \item[\autoref{alg:baum-erzeugen-c} \& \autoref{alg:baum-erzeugen-d}] Um eine zu starke Spezialisierung auf die Trainingsdaten zu verhindern, kann eine \textit{Genauigkeit} angegeben werden.
    Ist für eine Beispielmenge diese Genauigkeit erreicht so wird diese Beispielmenge nicht weiter unterteilt.
\end{description}

\begin{algorithm}
    \caption{Erzeugen eines Baums}
    \label{alg:baum-erzeugen}
   % \underline{function Euclid} $(a,b)$\;
    \Input{Beispielmenge $B$, Attributmenge $A$,\\
            Defaultwahrscheinlichkeit $d$ (in $\%$), Genauigkeit $g$}
    \Output{Entscheidungsbaum $E$}
    \eIf{$B = \emptyset$}{ \label{alg:baum-erzeugen-a}
        Gib Knoten mit Wahrscheinlichkeit $d$ zurück\;
        }{
            Setze $p$ = Anteil von positiven Klassifikationen in $b$\;
            \eIf{\\
                \hspace*{1.5em}$A = \emptyset$ {\normalfont oder} \label{alg:baum-erzeugen-b} \\
                \hspace*{1.5em}$p \geq g$ {\normalfont oder} \label{alg:baum-erzeugen-c}\\
                \hspace*{1.5em}$p \leq (1 - g)$ \label{alg:baum-erzeugen-d}
             \\}{
                Gib Knoten mit Wahrscheinlichkeit $p$ zurück\;
            }{
                Wähle $a \in A$ \label{alg:baum-erzeugen-d}\;
                Setze $A' = A \setminus \{a\}$\;
                Initialisiere $N$ als Unterbäume mit Kanten $w \in Werte(a)$\;
                \ForEach{$w_i \in Werte(a)$}{
                    Setze $B_i = \left\{b \in B | b.a = w_i\right\}$\;
                    Setze $N_{w_i} = Entscheidungsbaum(B_i, A', p, g)$\;
                }
                Gib Baum mit Wurzelknoten $a$ und Unterbäumen $N$ zurück\;
            }
        }
\end{algorithm}

Die Auswahl des als nächsten zu betrachtenden Attributs geschieht mittels \autoref{alg:auswahl-merkmal}.
Hierdurch wird das Attribut mit dem höchsten normierten Informationsgewinn ($Gain$) ausgewählt.
Mathematisch basiert diese Funktion auf der Gleichung,
welche in der Vorlesung vorgestellt wurde.

\begin{algorithm}
    \caption{Auswahl des besten Attributs}
    \label{alg:auswahl-merkmal}
    \Input{Beispielmenge $B$, Attributmenge $A$}
    \Output{Attribut $a$}
    Setze Gewinn $g = 0$\;
    \ForEach{$a_i \in A$}{
        Berechne $g_i = Gain(B,a_i)$\;
        \If{
            $g_i > g$
        }{
            Setze $g = g_i$\;
            Setze $a = a_i$\;
        }
    }
    Gib $a$ zurück\;
\end{algorithm}

\subsection{Implementierung}

Das Programm implementiert den zuvor beschriebenen Algorithmus in der Programmiersprache Java.
Lediglich um Kommandozeilenparameter angenehmer einzupflegen wird eine Bibliothek verwendet.

\subsubsection{Ausführen des Programms}
\todoask[inline]{Anderer Dateiname?}
Im Anhang an diese Dokument befindet sich die Datei \enquote{tree.jar}.
Grundlegend kann das Programm, wie in \autoref{lst:jar-aufruf} gezeigt, aufgerufen werden.
Erforderlich ist hierfür zumindest Java in der Version 8.

\begin{lstlisting}[
    caption=Einfacher Aufruf des Programms,
    label=lst:jar-aufruf,
    language=bash
]
java -jar tree.jar
\end{lstlisting}

Durch verschiedene Parameter kann das Programm beeinflusst werden:
Die Parameter und ihre Standartwerte sind in folgdender Übersicht dargestellt.

\begin{description}
    \setlength\itemsep{-0.5em}
    \item[\texttt{-{}-teach, -t}]
        Datei, welche zum Trainieren genutzt wird\\
        Standard: \texttt{Wohnungskartei\_Muster\_Master\_5\_K\_teach.csv}
    \item[\texttt{-{}-check, -c}]
        Datei, welche mit dem Baum geprüft wird\\
        Standard: <leer>
    \item[\texttt{-{}-print, -p}]
        Wenn gesetzt, wird der Baum wie in \autoref{appendix:baum} dargestellt\\
        Standard: false
    \item[\texttt{-{}-default, -d}]
        Initiale Defaultwahrscheinlichkeit\\
        Standard: 0,5
    \item[\texttt{-{}-accuracy, -a}]
        Genauigkeit bei welcher die Konstruktion des Unterbaums gestoppt wird.\\
        Standard: 1.0
    \item[\texttt{-{}-use-saved-tree, -u}]
        Wenn gesetzt wird der Baum aus der angegebenen Datei geladen und nicht neu erzeugt.\\
        Standard: <leer>
    \item[\texttt{-{}-save-tree, -s}]
        Speichert den neu trainierten Baum in der angegebenen Datei.\\
        Standard: <leer>
    \item[\texttt{-{}-help}]
        Gibt die Hilfe in der Kommandozeile aus
\end{description}



\subsubsection{Besondere Aspekte}
\todo[inline]{get Funktion von Datensatz}
\todo[inline]{Attribute als Enum mit Werten}
\todo[inline]{Bereinigung der Eingabedaten}
\todo[inline]{Insgesamt auf Anwendungsfall Spezialisiert > Bewertung}
