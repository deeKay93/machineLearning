\newpage
\section{Entscheidungsbaum}\label{sec:entscheidungsbaum}
Grundlegend basiert der Algorithmus zum Erzeugen des Entscheidungsbaums auf dem
in der Vorlesung vorgestellten Verfahren \emph{ID3}.
Ebenfalls wird die Verbesserung der Attributwahl durch den normierten Informationsgewinn übernommen.
Ergänzt um weitere Optimierungen entsteht so das in \autoref{alg:baum-erzeugen} beschriebene Verfahren.
Besonderer Augenmerk liegt hierbei auf den folgenden Anpassungen:
\begin{description}
    \item[\autoref{alg:baum-erzeugen-a}] Anstatt einer binären \enquote{Ja-Nein}-Klassifikation wird als Ergebnis die Wahrscheinlichkeit dafür zurückgegben,
                    dass für den entsprechenden Datensatz das Ergebnis positiv ist.
                    So können mehrere Datensätze bewertet und in einer Rangliste sortiert werden.
                    Beispielsweise könnte ein Makler bevorzugt wahrscheinlichere Mietobjekte anbieten.
    \item[\autoref{alg:baum-erzeugen-b}] Für den Fall, dass die Beispielmenge zwei (oder mehr) identische Datensätze mit unterschiedlicher Klassifikation enthält,
    jedoch die Attributmenge leer ist, wird der Anteil der positiven Klassifikationen als Wert für den entsprechenden Knoten gesetzt.
    \item[\autoref{alg:baum-erzeugen-c}] Enthält die Beispielmenge nur noch positive oder nur noch negative Ergebnisse,
        so wird der entsprechende Wert als Klassifikation für den Knoten gesetzt.
    \item[\autoref{alg:baum-erzeugen-d}] Um zu vermeiden, dass der Baum zu stark auf die Trainingsdaten spezialisiert,
        wird das Verhältnis zwischen Beispielmenge und den Werten des Attributs geprüft.
        Ist dieses Verhältnis kleiner, als der eingegebene Parameter, so endet die Erstellung des Baumes.

\end{description}

\begin{algorithm}
    \caption{Erzeugen eines Baums}
    \label{alg:baum-erzeugen}
   % \underline{function Euclid} $(a,b)$\;
    \Input{Beispielmenge $B$, Attributmenge $A$,\\
            Defaultwahrscheinlichkeit $d$ (in $\%$), Verhältnis $v$}
    \Output{Entscheidungsbaum $E$}
    \eIf{$B = \emptyset$}{
        Gib Knoten mit Wahrscheinlichkeit $d$ zurück\; \label{alg:baum-erzeugen-a}
        }{
            Setze $p$ = Anteil von positiven Klassifikationen in $b$\;
            \eIf{\\
                \hspace*{1.5em}$A = \emptyset$ {\normalfont oder} \label{alg:baum-erzeugen-b} \\
                \hspace*{1.5em}$p = 1 $ {\normalfont oder} $p = 0$ \label{alg:baum-erzeugen-c}
             \\}{
                Gib Knoten mit Wahrscheinlichkeit $p$ zurück\;
            }{
                Wähle $a \in A$\;
                \eIf{
                    $\frac{\vert B \vert}{\vert Werte(a) \vert} < v$ \label{alg:baum-erzeugen-d}
                }{
                    Gib Knoten mit Wahrscheinlichkeit $p$ zurück\;
                }{
                    Setze $A' = A \setminus \{a\}$\;
                    Initialisiere $N$ als Unterbäume mit Kanten $w \in Werte(a)$\;
                    \ForEach{$w_i \in Werte(a)$}{
                        Setze $B_i = \left\{b \in B | b.a = w_i\right\}$\;
                        Setze $N_{w_i} = Entscheidungsbaum(B_i, A', p, v)$\;
                    }
                    Gib Baum mit Wurzelknoten $a$ und Unterbäumen $N$ zurück\;
                }
            }
        }
\end{algorithm}

\noindent
Die Auswahl des als nächsten zu betrachtenden Attributs geschieht mittels \autoref{alg:auswahl-merkmal},
welche das Attribut mit dem höchsten normierten Informationsgewinn ($Gain$) auswählt.
Mathematisch basiert diese Funktion auf der Gleichung,
welche in der Vorlesung vorgestellt wurde.

\begin{algorithm}
    \caption{Auswahl des besten Attributs}
    \label{alg:auswahl-merkmal}
    \Input{Beispielmenge $B$, Attributmenge $A$}
    \Output{Attribut $a$}
    Setze Gewinn $g = 0$\;
    \ForEach{$a_i \in A$}{
        Berechne $g_i = Gain(B,a_i)$\;
        \If{
            $g_i > g$
        }{
            Setze $g = g_i$\;
            Setze $a = a_i$\;
        }
    }
    Gib $a$ zurück\;
\end{algorithm}