\newpage
\section{Entscheidungsbaum}\label{sec:entscheidungsbaum}
\todoask[inline]{Vorteile Entscheidungsbaum? / Einleitung?}

Grundlegend basiert der Algorithmus zum Erzeugen des Entscheidungsbaums auf dem,
in der Vorlesung vorgestellten. An verschiedenen Stellen wird der Algorithmus optimiert.
Der finale Algorithmus ist in \autoref{alg:baum-erzeugen} dargestellt.
Besonderer Augenmerk liegt hierbei auf den Optimierungen, welche im Folgenden beschrieben werden:
\begin{description}
    \item[\autoref{alg:baum-erzeugen-a}] Anstatt einer binären \enquote{Ja-Nein}-Klassifikation wird als Ergebnis die Wahrscheinlichkeit dafür zurückgegben,
                    dass für den entsprechenden Datensatz das Ergebnis positiv ist.
                    \todoask[inline]{Vorteil Wahrscheinlichkeit?}

    \item[\autoref{alg:baum-erzeugen-b}] Für den Fall, dass die Beispielmenge zwei (oder mehr) identische Datensätze mit unterschiedlicher Klassifikation enthält,
    jedoch die Attributmenge leer ist, wird der Anteil der positiven Klassifikationen als Wert für den entsprechenden Knoten gesetzt.
    \item[\autoref{alg:baum-erzeugen-c}] Enthält die Beispielmenge nur noch positive oder nur noch negative Ergebnisse,
        so wird der entsprechende Wert als Klassifikation für den Knoten gesetzt.
    \item[\autoref{alg:baum-erzeugen-d}] Um zu vermeiden, dass der Baum zu stark auf die Trainingsdaten spezialisiert,
        wird das Verhältnis zwischen Beispielmenge und den Werten des Attributs geprüft.
        Ist dieses Verhältnis kleiner, als der eingegebene Parameter, so endet die Erstellung des Baumes.

\end{description}

\begin{algorithm}
    \caption{Erzeugen eines Baums}
    \label{alg:baum-erzeugen}
   % \underline{function Euclid} $(a,b)$\;
    \Input{Beispielmenge $B$, Attributmenge $A$,\\
            Defaultwahrscheinlichkeit $d$ (in $\%$), Verhältnis $v$}
    \Output{Entscheidungsbaum $E$}
    \eIf{$B = \emptyset$}{ \label{alg:baum-erzeugen-a}
        Gib Knoten mit Wahrscheinlichkeit $d$ zurück\;
        }{
            Setze $p$ = Anteil von positiven Klassifikationen in $b$\;
            \eIf{\\
                \hspace*{1.5em}$A = \emptyset$ {\normalfont oder} \label{alg:baum-erzeugen-b} \\
                \hspace*{1.5em}$p = 1 $ {\normalfont oder} $p = 0$ \label{alg:baum-erzeugen-c}
             \\}{
                Gib Knoten mit Wahrscheinlichkeit $p$ zurück\;
            }{
                Wähle $a \in A$\;
                \eIf{
                    $\frac{\vert B \vert}{\vert Werte(a) \vert} < v$ \label{alg:baum-erzeugen-d}
                }{
                    Gib Knoten mit Wahrscheinlichkeit $p$ zurück\;
                }{
                    Setze $A' = A \setminus \{a\}$\;
                    Initialisiere $N$ als Unterbäume mit Kanten $w \in Werte(a)$\;
                    \ForEach{$w_i \in Werte(a)$}{
                        Setze $B_i = \left\{b \in B | b.a = w_i\right\}$\;
                        Setze $N_{w_i} = Entscheidungsbaum(B_i, A', p, v)$\;
                    }
                    Gib Baum mit Wurzelknoten $a$ und Unterbäumen $N$ zurück\;
                }
            }
        }
\end{algorithm}

Die Auswahl des als nächsten zu betrachtenden Attributs geschieht mittels \autoref{alg:auswahl-merkmal}.
Hierdurch wird das Attribut mit dem höchsten normierten Informationsgewinn ($Gain$) ausgewählt.
Mathematisch basiert diese Funktion auf der Gleichung,
welche in der Vorlesung vorgestellt wurde.

\begin{algorithm}
    \caption{Auswahl des besten Attributs}
    \label{alg:auswahl-merkmal}
    \Input{Beispielmenge $B$, Attributmenge $A$}
    \Output{Attribut $a$}
    Setze Gewinn $g = 0$\;
    \ForEach{$a_i \in A$}{
        Berechne $g_i = Gain(B,a_i)$\;
        \If{
            $g_i > g$
        }{
            Setze $g = g_i$\;
            Setze $a = a_i$\;
        }
    }
    Gib $a$ zurück\;
\end{algorithm}